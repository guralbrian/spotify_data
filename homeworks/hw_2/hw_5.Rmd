---
title: "BIOS 611: Clustering and Dimensionality Reduction"
author: "Brian Gural"
date: "2023-11-29"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(R.utils)
library(ggplot2)
library(dplyr)
library(scales)
```

## Problem 1: Principal Component Analysis (PCA)

### Load data into R

```{r unzip}
library(R.utils)
gunzip("~/homeworks/hw_5/data/embeddings_output_clean.csv.gz", remove=FALSE)
```

```{r load}
data <- read.csv("~/homeworks/hw_5/data/embeddings_output_clean.csv", header = F)
```

### Perform PCA on the dataset

```{r pca comp}
pca <- prcomp(data)
```
```{r p pca var}
# Make data frame with individual and cumulative variance explained by each PC
pc.df <- data.frame(sdev = pca$sdev) |> 
         mutate(ind.var = sdev^2/sum(sdev^2),
                tot.var = cumsum(ind.var),
                pc = seq(1:length(sdev)))

# Individual variance explained by each PC
  ggplot(pc.df) +
   geom_point(aes(x = pc, y = ind.var), size = 6, color = "salmon") +
   scale_x_continuous(breaks=c(1:10), labels = c(1:10), limits = c(1,10)) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "Principal Component #",
        y = "Individual Variance Explained",
        title = "PCA of UFO data shows minimal variance explained by PCs > 5")
# Cumulative variance explained by each PC
  ggplot(pc.df) +
   geom_point(aes(x = pc, y = tot.var), size = 3, color = "salmon") +
   scale_x_continuous(breaks=seq(0,500,100), labels = seq(0,500,100), limits = c(1,500)) +
   scale_y_continuous(limits = c(0,1)) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "Principal Component #",
        y = "Cumulative Variance Explained",
        title = "PCA of UFO data shows 90% of total variance explained by ~200 PCs")
         
```

The two plots show that the majority of the variance in the data is explained by the first few principal components. If we were to use the elbow method, the optimal number of PCs to include in downstream analysis is 4 - 5, per the individual variance plot. In other words, PCs 6 - 4096 aren't likely to help explain trends in the data set.

## Problem 2: Cluster Analysis in PCA Space

### Create a scatter plot of the first principal component (PC1) versus the second principal component (PC2)

```{r p pca 2}
pc.rot <- pca$x |> as.data.frame()

# PC1 by PC2
ggplot(pc.rot) +
   geom_point(aes(x = PC1, y = PC2), size = 2, color = "sienna2", alpha = 0.4) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "PC1",
        y = "PC2") +
  labs(title = "UFO summary data PCs show three clusters",
       subtitle = "Cluster 1: Large banana in center of plot\nCluster 2: Located at (-55,5)\nCluster 3: Located at (110,-65)")
         
```
### The PCA results are in the same row order as the original UFO dataset. By filtering on PC1 and PC2, you can identify corresponding indices in the original UFO dataset.

```{r load old data, eval = FALSE}
data.old <- read.csv("https://raw.githubusercontent.com/Vincent-Toups/611ufo/main/source_data/ufo_data.csv")

# remove duplicate rows
data.old <- data.old |> 
        group_by(Summary) 

```
```{r merge pc data }
pc.merged <- cbind(pc.rot[,c(1,2)], data.old)

# PC1 by PC2
ggplot(pc.merged) +
   geom_point(aes(x = PC1, y = PC2, color = lng), size = 2, alpha = 0.4) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "PC1",
        y = "PC2") +
  labs(title = "UFO summary data PCs show three clusters",
       subtitle = "Cluster 1: Large banana in center of plot\nCluster 2: Located at (-55,5)\nCluster 3: Located at (110,-65)")


```

```{r make clusters}
classify_cluster <- function(PC1, PC2) {
       if (PC1 < -30) {
    return('Cluster 1') # Large banana in center
  } else if (PC1 > 100 && PC2 < -50) {
    return('Cluster 2') # Group on bottom left
  } else {
    return('Cluster 3') # For any points that don't clearly belong to any cluster
  }
}

# Apply the classification function to each row
pc.merged <- pc.merged %>%
  rowwise() %>%
  mutate(Cluster = classify_cluster(PC1, PC2)) %>%
  ungroup()



```
```{r clean times}

library(lubridate)
library(dplyr)
library(stringr)

# Inspect the dates that failed to parse
unparsed_dates <- pc.merged %>%
  mutate(parsed_date = parse_date_time(Date...Time, orders = c("mdy HM", "ymd HMS"))) %>%
  filter(is.na(parsed_date))

# View some of the unparsed dates to identify their formats
print(head(unparsed_dates$Date...Time))

# Once you've identified the additional formats, add them to the 'orders' vector
# For example, let's assume you found a format like "yyyy/mm/dd HH:MM"
additional_formats <- c("mdy HM", "ymd HMS", "ymd HM", "y/m/d HM")

# Now, re-run the mutate with the expanded list of formats
pc.merged <- pc.merged %>%
  mutate(Date...Time = case_when(
    str_detect(Date...Time, "^\\d{2}/\\d{2}/\\d{2}") ~ mdy_hm(Date...Time),  # Format: "mm/dd/yy hh:mm"
    str_detect(Date...Time, "^\\d{4}-\\d{2}-\\d{2}") ~ ymd_hms(Date...Time), # Format: "yyyy-mm-dd hh.mm.ss"
    TRUE ~ parse_date_time(Date...Time, orders = additional_formats)  # Fallback for other formats
  ))

# Check the results
head(pc.merged$Date...Time)

```
```{r plot clusters}
ggplot(pc.merged) +
   geom_point(aes(x = PC1, y = PC2, color = Cluster), size = 2, alpha = 0.4) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "PC1",
        y = "PC2") +
  labs(title = "UFO summary data PCs show three clusters",
       subtitle = "Cluster 1: Large banana in center of plot\nCluster 2: Located at (-55,5)\nCluster 3: Located at (110,-65)")


ggplot(pc.merged) +
   geom_density(aes(x = Date...Time, fill = Cluster), alpha = 0.5) +
   theme_minimal() +
   theme(panel.grid.minor = element_blank()) +
   labs(x = "Time-of-day",
        y = "Density") 

```
## Problem 3

**Question**

Given the following code:

```{r, eval = FALSE}
library(R.utils)
gunzip("file.gz", remove=FALSE)
```

**Answer**

